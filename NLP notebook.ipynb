{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86b65dd3",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Natural Language Processing (Compiled by Pawan Neupane)\n",
    "\n",
    "## Topics covered\n",
    "*** \n",
    "#### 1) Tokenization \n",
    "#### 2) Stemming\n",
    "#### 3) Lemmatization \n",
    "#### 4) Stop Words\n",
    "#### 5) Bag of words\n",
    "#### 6) TFIDF\n",
    "#### 7) Ngrams\n",
    "#### 8) wrd2vec\n",
    "#### 9) NER\n",
    "#### 10) Text classification example\n",
    "#### 11) RNN\n",
    "#### 12) LSTM\n",
    "#### 13) Transformers\n",
    "#### 14) BERT\n",
    "#### 15) GPT\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755edbf9",
   "metadata": {},
   "source": [
    "## 1) Tokenization\n",
    "Splitting a larger body of text into small, manageable pieces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d7e4b4",
   "metadata": {},
   "source": [
    " <img src=\"images/tokenization.png\" style= \"width:500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38954fa8",
   "metadata": {},
   "source": [
    "### Tokenization using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a04ace42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4382facc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pawanneupane/opt/anaconda3/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm') #loading a small sized english library for convenience "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afaabc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytext = \"I live in Kathmandu NP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bad521af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "live\n",
      "in\n",
      "Kathmandu\n",
      "NP\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(mytext)\n",
    "\n",
    "for d in doc:\n",
    "    print(d)\n",
    "    \n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906a37c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da54312",
   "metadata": {},
   "source": [
    "### Tokenization using Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60ef3b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88fea0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', '!', 'How', 'are', 'you', '?']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytext = \"Hello! How are you?\"\n",
    "\n",
    "tokenizer.tokenize(mytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a1de8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1628495c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f9420d4",
   "metadata": {},
   "source": [
    "## 2) Stemming\n",
    "Truncation the root word from adjectives adverbs etc(removal of parts like ing, ly, ies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fe309e",
   "metadata": {},
   "source": [
    " <img src=\"images/stemming.png\" style= \"width:500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "adf3e8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "s_stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "959f20ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go => go\n",
      "going => go\n",
      "runs => run\n",
      "ran => ran\n",
      "running => run\n",
      "easily => easili\n",
      "fairly => fair\n"
     ]
    }
   ],
   "source": [
    "words = ['go','going','runs','ran','running','easily','fairly']\n",
    "\n",
    "for word in words:\n",
    "    print(word+' => '+s_stemmer.stem(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce8217b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5420ddbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19ba509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42de22e5",
   "metadata": {},
   "source": [
    "## 3) Lemmatization\n",
    "\n",
    "More advanced than stemming since it can discern different tenses, like saw->see"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f46572",
   "metadata": {},
   "source": [
    " <img src=\"images/lemmatization.png\" style= \"width:500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2d179936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_lemmas(text):\n",
    "    print(\"Word         POS    Token Lemma Hash       Root Lemma\")\n",
    "    print(\"\\n\")\n",
    "    for token in text:\n",
    "        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "0d0dcd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word         POS    Token Lemma Hash       Root Lemma\n",
      "\n",
      "\n",
      "I            PRON   4690420944186131903    I\n",
      "saw          VERB   11925638236994514241   see\n",
      "eighteen     NUM    9609336664675087640    eighteen\n",
      "mice         NOUN   1384165645700560590    mouse\n",
      "today        NOUN   11042482332948150395   today\n",
      "!            PUNCT  17494803046312582752   !\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(u\"I saw eighteen mice today!\")\n",
    "\n",
    "show_lemmas(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39523f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479e2c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fa6a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6edf7d00",
   "metadata": {},
   "source": [
    "## 4) Stop words\n",
    "Commonly used words with no important meanings whatsoever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "939b3160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326\n",
      "['between', 'someone', 'would', 'through', \"'ll\", 'anyone', 'whereas', 'his', 'ours', 'moreover', 'until', '’s', 'thereby', 'besides', 'just', 'toward', '’ll', 'always', 'mine', \"'s\", 'down', '’m', 'wherein', 'as', 'almost', 'will', 'give', 'back', 'seemed', 'former', 'n‘t', 'were', 'thereafter', 'what', 'further', 'nobody', 'herself', 'was', 'across', 'below', '‘re', 'become', 'from', 'if', 'beyond', 'whereupon', 'without', 'nothing', 'any', 'less', 'show', 'n’t', 'other', 'eight', 'using', 'himself', '’re', 'each', 'could', 'formerly', 'why', 'nevertheless', 'eleven', 'four', 'otherwise', 'or', 'now', 'hereupon', 'has', 'bottom', 'whereafter', 'thereupon', 'whereby', 'into', 'more', 'hereby', 'much', 'via', 'somehow', 'indeed', '’ve', 'made', 'out', 'something', 'you', 'doing', 'everyone', 'also', 'else', 'how', 'towards', 'due', 'done', 'next', 'though', 'whose', 'every', 'them', 'fifteen', 'but']\n"
     ]
    }
   ],
   "source": [
    "#View default stopwords in spacy\n",
    "print(len(nlp.Defaults.stop_words))\n",
    "print(list(nlp.Defaults.stop_words)[:100]) #changing set to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6dcc9612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['kathmandu'].is_stop #check if Kathmandu is a stop word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692ee70d",
   "metadata": {},
   "source": [
    "### Stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "f653bd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rabin', 'likes', 'play', 'cricket', ',', 'football', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Rabin likes to play cricket, however he is not into football.\"\n",
    "text_tokens = word_tokenize(text)\n",
    "\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "a15d670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24e78e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961c5a12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc66d56a",
   "metadata": {},
   "source": [
    "## 5) Bag of Words\n",
    "Collection of all the words with their respective counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2637fbe",
   "metadata": {},
   "source": [
    " <img src=\"images/bagofwords.png\" style= \"width:500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "6282f4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brown', 'down', 'fox', 'is', 'jumps', 'mountain', 'over', 'river', 'runs', 'the']\n",
      "1\t      0\t      1\t      0\t      1\t      1\t      1\t      0\t      0\t      2\t      \n",
      "\n",
      "0\t      0\t      0\t      1\t      0\t      1\t      1\t      1\t      0\t      1\t      \n",
      "\n",
      "0\t      1\t      0\t      0\t      0\t      1\t      0\t      1\t      1\t      1\t      \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    " \n",
    "doc_list = [\"the Brown fox jumps over the mountain\",\n",
    "            \"the mountain is over a river\",\n",
    "            \"River runs down the mountain\"]\n",
    "\n",
    "vectorizer = CountVectorizer() #creating an instance of countvectorizer\n",
    "\n",
    "vectorizer.fit(doc_list)\n",
    "vector = vectorizer.transform(doc_list)\n",
    "lis = sorted(vectorizer.vocabulary_)\n",
    "print(lis)\n",
    "\n",
    "\n",
    "# print(vector.toarray())\n",
    "list_vector = vector.toarray()\n",
    "\n",
    "for i in list_vector:\n",
    "    for j in i:\n",
    "        print(j, end='\\t      ')\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf4e20d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0784b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f623585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9de5741a",
   "metadata": {},
   "source": [
    "## 6) TFIDF : Term frequency Inverse Document Frequency \n",
    "\n",
    "Considers the occurence of the words throughout the document instead of focusing just a single line. Superior to BagofWords(Countvectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482bd710",
   "metadata": {},
   "source": [
    " <img src=\"images/tfidf.png\" style= \"width:500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a43533b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brown', 'down', 'fox', 'is', 'jumps', 'mountain', 'over', 'river', 'runs', 'the']\n",
      "0.43345167 0.0 0.43345167 0.0 0.43345167 0.25600354 0.32965117 0.0 0.0 0.51200708 \n",
      "\n",
      "0.0 0.0 0.0 0.59188659 0.0 0.34957775 0.45014501 0.45014501 0.0 0.34957775 \n",
      "\n",
      "0.0 0.55249005 0.0 0.0 0.0 0.32630952 0.0 0.42018292 0.55249005 0.32630952 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " \n",
    "doc_list = [\"the Brown fox jumps over the mountain\",\n",
    "            \"the mountain is over a river\",\n",
    "            \"River runs down the mountain\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer() #creating an instance of countvectorizer\n",
    "\n",
    "vectorizer.fit(doc_list)\n",
    "vector = vectorizer.transform(doc_list)\n",
    "lis = sorted(vectorizer.vocabulary_)\n",
    "print(lis)\n",
    "\n",
    "\n",
    "# print(vector.toarray())\n",
    "list_vector = vector.toarray()\n",
    "\n",
    "for i in list_vector:\n",
    "    for j in i:\n",
    "        print(round(j,8), end=' ')\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c1f6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a77397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb592b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cba2a2f3",
   "metadata": {},
   "source": [
    "## 7) N-Grams\n",
    "\n",
    "Grouping of words into 1-gram, 2-gram, 4-gram etc. according to the requirements. For example, according to a study, Spam detection works best with 4-gram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16307fe2",
   "metadata": {},
   "source": [
    " <img src=\"images/ngrams.png\" style= \"width:500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "e9842933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.util import ngrams\n",
    "\n",
    "s = \"the Brown fox-jumps over the mountain\"\n",
    "s = s.lower()\n",
    "s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "output = list(ngrams(tokens, 3)) #change the number to change n of ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "af623540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 'brown', 'fox')\n"
     ]
    }
   ],
   "source": [
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1283e9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e36f20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a40f008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "635e4b10",
   "metadata": {},
   "source": [
    "## 8) Word2Vec | Average Word2Vec\n",
    "\n",
    "converting a word to vector with a lot of dimenstions (100-1000 normally) so as to perform mathematical operations\n",
    "\n",
    "#### Average word2vec is average of all vectors in a sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020813ca",
   "metadata": {},
   "source": [
    " <img src=\"images/word2vec.png\" style= \"width:500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "503c5c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "print(nlp.vocab.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cc2e127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp(u'home').vector\n",
    "\n",
    "# len(nlp.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70f9a9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "animals = nlp(u\"dog pet eagle cat\")\n",
    "print(animals[3].similarity(animals[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca0e5021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomaly = nlp(\"dsdjspwpd\")\n",
    "anomaly[0].has_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7ee3d91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['king', 'woman', 'she', 'who', 'eagle', 'when', 'dare', 'cat', 'was', 'not']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy import spatial\n",
    "\n",
    "cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n",
    "\n",
    "king = nlp.vocab['king'].vector\n",
    "man = nlp.vocab['man'].vector\n",
    "woman = nlp.vocab['woman'].vector\n",
    "\n",
    "# Now we find the closest vector in the vocabulary to the result of \"man\" - \"woman\" + \"queen\"\n",
    "new_vector = king - man + woman\n",
    "computed_similarities = []\n",
    "for word in nlp.vocab:\n",
    "    if word.has_vector:\n",
    "        if word.is_lower:\n",
    "            if word.is_alpha:\n",
    "#                 print(word)\n",
    "                similarity = cosine_similarity(new_vector, word.vector)\n",
    "                computed_similarities.append((word, similarity))\n",
    "\n",
    "computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\n",
    "\n",
    "print([w[0].text for w in computed_similarities[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "94ed8fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Averege word2vec\n",
    "\n",
    "\n",
    "#nlp(u'this is a nice line').vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af2144d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1701e4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992a0fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5a33505",
   "metadata": {},
   "source": [
    "## 9) NER | Named Entity Recognition \n",
    "Find where the specific parts of a text belong. For example: Organizations, Products, Countries etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eba393",
   "metadata": {},
   "source": [
    " <img src=\"images/ner.png\" style= \"width:500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1ea3d2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 dollars 4 6 20 31 MONEY\n",
      "Microsoft 11 12 53 62 ORG\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(u'Can I please borrow 500 dollars from you to buy some Microsoft stock?')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start, ent.end, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "73d409ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eede5a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Over \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the last quarter\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " sold \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    nearly 20 thousand\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " iPods for a profit of \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $6 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ". By contrast, \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sony\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " sold \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    only 7 thousand\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Walkman\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       " music players.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualizing\n",
    "doc = nlp(u'Over the last quarter Apple sold nearly 20 thousand iPods for a profit of $6 million. '\n",
    "         u'By contrast, Sony sold only 7 thousand Walkman music players.')\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf6e164c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50daf3de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc55d698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "999b0880",
   "metadata": {},
   "source": [
    "## 10) Text Classification (Movie Reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "28b1375e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                             review\n",
      "0   pos  I loved this movie and will watch it again. Or...\n",
      "1   pos  A warm, touching movie that has a fantasy-like...\n",
      "2   pos  I was not expecting the powerful filmmaking ex...\n",
      "3   neg  This so-called \"documentary\" tries to tell tha...\n",
      "4   pos  This show has been my escape from reality for ...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('moviereviews.tsv', sep='\\t')\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b3316ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos    2990\n",
       "neg    2990\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8a1550d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, LinearSVC())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, LinearSVC())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', LinearSVC())])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "X = df['review']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('clf', LinearSVC()),\n",
    "])\n",
    "\n",
    "# Feed the training data through the pipeline\n",
    "text_clf.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7a17ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form a prediction set\n",
    "\n",
    "predictions = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8fb5bfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[900  91]\n",
      " [ 63 920]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.93      0.91      0.92       991\n",
      "         pos       0.91      0.94      0.92       983\n",
      "\n",
      "    accuracy                           0.92      1974\n",
      "   macro avg       0.92      0.92      0.92      1974\n",
      "weighted avg       0.92      0.92      0.92      1974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test,predictions))\n",
    "\n",
    "print(metrics.classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "feda42f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9219858156028369\n"
     ]
    }
   ],
   "source": [
    "# Print the overall accuracy\n",
    "print(metrics.accuracy_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1f6dbd59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = text_clf.predict([\"this is awful\"])\n",
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1784d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17781f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca1c0c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f1b02c6",
   "metadata": {},
   "source": [
    "## 11) RNN\n",
    "\n",
    "Disadvantages:\n",
    "* Slow\n",
    "* Vanishing gradient\n",
    "* Doesnot work well for long sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c234b0a9",
   "metadata": {},
   "source": [
    " <img src=\"images/RNN1.png\" style= \"width:500px\">\n",
    "     <br>\n",
    "     <img src=\"images/RNN2.png\" style= \"width:500px\">\n",
    "         <br>\n",
    "         <img src=\"images/RNN3.png\" style= \"width:500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7658235d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675721b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf1f38c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4746a6d",
   "metadata": {},
   "source": [
    "## 12) LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aff7f8",
   "metadata": {},
   "source": [
    " <img src=\"images/LSTM.png\" style= \"width:500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463d9431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7634dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d3089a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f397c232",
   "metadata": {},
   "source": [
    "## 13) Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef930fa",
   "metadata": {},
   "source": [
    " <img src=\"images/transformer.png\" style= \"width:500px\">\n",
    " <img src=\"images/transformer1.png\" style= \"width:500px\">\n",
    "  <img src=\"images/transformer2.png\" style= \"width:500px\">\n",
    "   <img src=\"images/transformer3.png\" style= \"width:500px\">\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8030037c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaee08a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dd28e93",
   "metadata": {},
   "source": [
    "## 14) BERT\n",
    "Stacking encoders together\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfc820a",
   "metadata": {},
   "source": [
    "<img src=\"images/bert.png\" style= \"width:500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9f192e",
   "metadata": {},
   "source": [
    "## 15) GPT\n",
    "Stacking decoders together\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb07f23",
   "metadata": {},
   "source": [
    "<img src=\"images/gpt.png\" style= \"width:500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dcc656",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
